{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff03c206-178a-4cbd-8184-dc81bf23b64a",
   "metadata": {},
   "source": [
    "# List of Libraries using in this notebook\n",
    "#### pandas: For data manipulation and analysis\n",
    "#### numpy: For numerical operations\n",
    "#### matplotlib: For data visualization\n",
    "#### seaborn: For data visualization\n",
    "#### wordcloud: For data visualization\n",
    "#### pickle: For saving and loading the model\n",
    "#### re: For regular expression\n",
    "#### string: For string operations\n",
    "#### nltk: For natural language processing\n",
    "#### nltk.util: which is used to generate bigrams and trigrams\n",
    "#### nltk.corpus:  which is used to get the list of stopwords\n",
    "#### sklearn: For machine learning algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67856e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/binurathiranjaya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.util import pr\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dddb46",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "#### nltk.download('stopwords'): To download the stopwords\n",
    "#### nltk.SnawballStemmer('english'): To get the stem of the word\n",
    "#### set(stopwords.words('english')): To get the list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663abb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "stopword=set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f254cc",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "# Read the data using pandas using read_csv() function\n",
    "# Display the first five rows of the data using head() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8af69d5d-8a7d-4123-a57c-ddec0ca6a35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"labeled_data.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4fbae4",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "# Clean the data by removing the missing values and url links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95d595c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet                 labels\n",
      "0   rt mayasolov woman shouldnt complain clean ho...  No Hate and Offensive\n",
      "1   rt  boy dat coldtyga dwn bad cuffin dat hoe  ...     Offensive Language\n",
      "2   rt urkindofbrand dawg rt  ever fuck bitch sta...     Offensive Language\n",
      "3             rt cganderson vivabas look like tranni     Offensive Language\n",
      "4   rt shenikarobert shit hear might true might f...     Offensive Language\n"
     ]
    }
   ],
   "source": [
    "data[\"labels\"] = data[\"class\"].map({0: \"Hate Speech\", \n",
    "                                    1: \"Offensive Language\", \n",
    "                                    2: \"No Hate and Offensive\"})\n",
    "data = data[[\"tweet\", \"labels\"]]\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    return text\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(clean)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d8e934-2d6e-4eb6-a1b7-4725ca0be7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tweet                 labels\n",
      "0    rt mayasolov woman shouldnt complain clean ho...  No Hate and Offensive\n",
      "40                momma said pussi cat insid doghous   No Hate and Offensive\n",
      "63      simplyaddictedtoguy  woof woof hot scalli lad  No Hate and Offensive\n",
      "66                allaboutmanfeet  woof woof hot sole  No Hate and Offensive\n",
      "67    allyhaaaaa lemmi eat oreo amp dish one oreo lol  No Hate and Offensive\n"
     ]
    }
   ],
   "source": [
    "# print No Hate and Offensive tweets\n",
    "non_hate_offensive_tweets = data[data[\"labels\"] == \"No Hate and Offensive\"]\n",
    "print(non_hate_offensive_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7263578-3671-4802-a68b-93272c01087e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet              labels\n",
      "1   rt  boy dat coldtyga dwn bad cuffin dat hoe  ...  Offensive Language\n",
      "2   rt urkindofbrand dawg rt  ever fuck bitch sta...  Offensive Language\n",
      "3             rt cganderson vivabas look like tranni  Offensive Language\n",
      "4   rt shenikarobert shit hear might true might f...  Offensive Language\n",
      "5  tmadisonx shit blow meclaim faith somebodi sti...  Offensive Language\n"
     ]
    }
   ],
   "source": [
    "# print Offensive Language tweets\n",
    "offensive_tweets = data[data[\"labels\"] == \"Offensive Language\"]\n",
    "print(offensive_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16bb27-a327-4e83-ac38-da323b02c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweet       labels\n",
      "85                         whalelookyher  queer gaywad  Hate Speech\n",
      "89    whitethunduh alsarabsss hes beaner smh tell h...  Hate Speech\n",
      "110  devilgrimz vigxrart your fuck gay blacklist ho...  Hate Speech\n",
      "184  markroundtreejr lmfaoooo hate black peopl  the...  Hate Speech\n",
      "202                   nochillpaz least im nigger lmfao  Hate Speech\n"
     ]
    }
   ],
   "source": [
    "# print Hate Speech tweets\n",
    "hate_tweets = data[data[\"labels\"] == \"Hate Speech\"]\n",
    "print(hate_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing hate and not hate tweets\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.countplot(data[\"labels\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6fb998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Hate Speech tweets\n",
    "hate_speech = data[data[\"labels\"] == \"Hate Speech\"]\n",
    "hate_speech = hate_speech[\"tweet\"]\n",
    "hate_speech = \" \".join(hate_speech)\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopword, \n",
    "                min_font_size = 10).generate(hate_speech)\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Offensive Language tweets\n",
    "offensive_language = data[data[\"labels\"] == \"Offensive Language\"]\n",
    "offensive_language = offensive_language[\"tweet\"]\n",
    "offensive_language = \" \".join(offensive_language)\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopword, \n",
    "                min_font_size = 10).generate(offensive_language)\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only No Hate and Offensive tweets\n",
    "no_hate_offensive = data[data[\"labels\"] == \"No Hate and Offensive\"]\n",
    "no_hate_offensive = no_hate_offensive[\"tweet\"]\n",
    "no_hate_offensive = \" \".join(no_hate_offensive)\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopword, \n",
    "                min_font_size = 10).generate(no_hate_offensive)\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ebe4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "x = np.array(data[\"tweet\"])\n",
    "y = np.array(data[\"labels\"])\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(x) # Fit the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7071da8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No Hate and Offensive']\n"
     ]
    }
   ],
   "source": [
    "sample = \"I am  a good boy\"\n",
    "data = cv.transform([sample]).toarray()\n",
    "print(clf.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b39e4cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Offensive Language' 'Offensive Language' 'Offensive Language' ...\n",
      " 'Offensive Language' 'Offensive Language' 'Offensive Language']\n",
      "Confusion Matrix: \n",
      " [[ 156   41  268]\n",
      " [  38 1124  217]\n",
      " [ 227  218 5890]]\n",
      "Classification Report: \n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "          Hate Speech       0.37      0.34      0.35       465\n",
      "No Hate and Offensive       0.81      0.82      0.81      1379\n",
      "   Offensive Language       0.92      0.93      0.93      6335\n",
      "\n",
      "             accuracy                           0.88      8179\n",
      "            macro avg       0.70      0.69      0.70      8179\n",
      "         weighted avg       0.87      0.88      0.88      8179\n",
      "\n",
      "Accuracy: \n",
      " 0.8766352854872234\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "predictions = clf.predict(X_test)\n",
    "print(predictions)\n",
    "\n",
    "# Evaluating the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, predictions))\n",
    "print(\"Accuracy: \\n\", accuracy_score(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38c83202-da58-4384-af69-6a34fe609784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No Hate and Offensive']\n"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "pickle.dump(clf, open(\"model.pkl\", \"wb\"))\n",
    "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))\n",
    "\n",
    "# Loading the model\n",
    "model = pickle.load(open(\"model.pkl\", \"rb\"))\n",
    "cv = pickle.load(open(\"cv.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "# Testing the model\n",
    "# predictions = model.predict(X_test)\n",
    "# print(predictions)\n",
    "\n",
    "# Evaluating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0facfcf-e41c-41ae-9a9b-2ad50c9cc302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hate Speech']\n"
     ]
    }
   ],
   "source": [
    "sample = \"I will kill you \" \n",
    "data = cv.transform([sample]).toarray()\n",
    "print(model.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bcd822-b0ff-4d86-ad90-0b38be51064a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c5dea-0950-43af-8bfd-e8127d3b7aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
